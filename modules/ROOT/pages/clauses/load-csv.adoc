:description: `LOAD CSV` is used to import data from CSV files.

= LOAD CSV

== Introduction

`LOAD CSV` is used to import data from CSV files.

The clause syntax is:

.Query
[source, cypher]
----
LOAD CSV FROM '<file_URL>' AS row
...
----

`FROM` expects a string representing the URL with the CSV file.
Specify a variable for the CSV data using `AS`.
This variable represents the current row while LOAD CSV iterates through the lines of the CSV file.
This way, you can access the data in subsequent clauses.

[NOTE]
====
Your user needs link:{neo4j-docs-base-uri}/operations-manual/{page-version}/authentication-authorization/load-privileges/[load privileges] to run `LOAD CSV` statements.
====

== Import CSV data into Neo4j

=== Import local files

You can store CSV files on the database server and then access them by using a `+file:///+` URL.
The following example loads the name and year information for a number of artists into the database:

.artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists.csv' AS row
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----


.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----

.Configuration settings for file URLs
link:{neo4j-docs-base-uri}/operations-manual/{page-version}/configuration/configuration-settings#config_dbms.security.allow_csv_import_from_file_urls[dbms.security.allow_csv_import_from_file_urls]::
This setting determines if Cypher allows the use of `+file:///+` URLs when loading data using `LOAD CSV`.

link:{neo4j-docs-base-uri}/operations-manual/{page-version}/configuration/configuration-settings#config_server.directories.import[server.directories.import]::
This setting sets the root directory relative to which `+file:///+` URLs are parsed.

When using `+file:///+` URLs, spaces and other non-alphanumeric characters must be link:https://developer.mozilla.org/en-US/docs/Glossary/percent-encoding[URL-encoded].

Note that since link:https://neo4j.com/cloud/platform/aura-graph-database/[AuraDB] is cloud based, this local file approach does not work with AuraDB.


=== Import from a remote location

You can import data from a CSV file in a remote location into Neo4j.

`LOAD CSV` supports accessing CSV files via _HTTPS_, _HTTP_, and _FTP_.
`LOAD CSV` follows _HTTP_ redirects but for security reasons it doesn't follow redirects which change the protocol.

.data.neo4j.com/bands/artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'https://data.neo4j.com/bands/artists.csv' AS row
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----


=== Access line numbers with `linenumber()`

The xref:functions/load-csv.adoc#functions-linenumber[`linenumber()`] function provides the line number which `LOAD CSV` is operating on, or `null` if called without a `LOAD CSV` context.

.artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists.csv' AS row
RETURN linenumber() AS number, row
----

.Result
[role="queryresult"]
----
+---------------------------------------+
| number | row                          |
+---------------------------------------+
| 1      | ["1","ABBA","1992"]          |
| 2      | ["2","Roxette","1986"]       |
| 3      | ["3","Europe","1979"]        |
| 4      | ["4","The Cardigans","1992"] |
+---------------------------------------+
4 rows
----


=== Access the CSV file path with `file()`

The xref:functions/load-csv.adoc#functions-file[`file()`] function provides the absolute path of the file that `LOAD CSV` is operating on, or `null` if called without a `LOAD CSV` context.

.artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher, role=test-result-skip]
----
LOAD CSV FROM 'file:///artists.csv' AS row
RETURN DISTINCT file() AS path
----

.Result
[role="queryresult"]
----
+------------------------------------------+
| path                                     |
+------------------------------------------+
| "/home/example/neo4j/import/artists.csv" |
+------------------------------------------+
1 row
----

[TIP]
====
`file()` always returns a local path, even when loading remote CSV files. For remote resources, `file()` returns the temporary local path it was downloaded to.
====


=== Import compressed CSV files

`LOAD CSV` can read local CSV files compressed with _ZIP_ or _gzip_.
ZIP archives to be used with `LOAD CSV` can have arbitrary directory structures but may only contain a single CSV file.

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists.zip' AS row
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----


=== Create constraints

Always create constraints prior to importing data.
The CSV files _persons.csv_ and _movies.csv_ contain IDs for people and movies:

.+persons.csv+
[source, csv, filename="persons.csv"]
----
person_tmdbId,bio,born,bornIn,died,person_imdbId,name,person_poster,person_url
3,"Legendary Hollywood Icon Harrison Ford was born on July 13, 1942 in Chicago, Illinois.   His family history includes a strong lineage of actors, radio personalities, and models.   Harrison attended public high school in Park Ridge, Illinois where he was a member of the school Radio Station WMTH.  Harrison worked as the lead voice for sports reporting at WMTH for several years.   Acting wasn’t a major interest to Ford until his junior year at Ripon College when he first took an acting class...",1942-07-13,"Chicago, Illinois, USA",,148,Harrison Ford,https://image.tmdb.org/t/p/w440_and_h660_face/5M7oN3sznp99hWYQ9sX0xheswWX.jpg,https://themoviedb.org/person/3
...
----

.+movies.csv+
[source, csv, filename="movies.csv"]
----
movieId,title,budget,countries,movie_imdbId,imdbRating,imdbVotes,languages,plot,movie_poster,released,revenue,runtime,movie_tmdbId,movie_url,year,genres
1,Toy Story,30000000.0,USA,114709,8.3,591836,English,A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.,https://image.tmdb.org/t/p/w440_and_h660_face/uXDfjJbdP4ijW5hWSBrPrlKpxab.jpg,1995-11-22,373554033.0,81,862,https://themoviedb.org/movie/862,1995,Adventure|Animation|Children|Comedy|Fantasy
2,Jumanji,65000000.0,USA,113497,6.9,198355,English|French,"When two kids find and play a magical board game, they release a man trapped for decades in it and a host of dangers that can only be stopped by finishing the game.",https://image.tmdb.org/t/p/w440_and_h660_face/vgpXmVaVyUL7GGiDeiK1mKEKzcX.jpg,1995-12-15,262797249.0,104,8844,https://themoviedb.org/movie/8844,1995,Adventure|Children|Fantasy
...
----

After import, they can identify a person or a movie node.
Neo4j's concept of constraints can ensure uniqueness.
With uniqueness constraints in place, trying to create a person node with an existing `tmdbId` or a movie node with an existing `movieId` raises an error and doesn't create the node.

There are also other xref:constraints/index.adoc[types of constraints].

To create xref:constraints/examples.adoc#constraints-examples-node-uniqueness[node property uniqueness constraints] for the two IDs:

.Query
[source, cypher]
----
CREATE CONSTRAINT Person_tmdbId IF NOT EXISTS
FOR (p:Person)
REQUIRE p.tmdbId IS UNIQUE

CREATE CONSTRAINT Movie_movieId IF NOT EXISTS
FOR (m:Movie)
REQUIRE m.movieId IS UNIQUE
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Added 2 constraints.
----


=== CSV data quality

Real-world data can be messy.
When you work with such CSV data, you may see values that need a clean-up or a transformation before you can use them with `LOAD CSV`.
Doing this early can avoid cleaning up data that has been imported incorrectly.


==== Inconsistent headers

The CSV header may be inconsistent with the data.
It can be missing, have too many columns or have a different delimiter.

Verify that the header matches the data in the file.
Adjust the formatting, delimiters or columns.


==== Extra or missing quotes

Standalone double or single quotes in the middle of non-quoted text or non-escaped quotes in quoted text can cause issues reading the file.
Either escape or remove stray quotes.
See <<Character escaping and quotes>>.


==== Special or newline characters

When dealing with special characters in a file, ensure they are quoted or remove them.
Either add quotes to newline characters or remove them from quoted or unquoted fields.


==== Inconsistent line breaks

Ensure line breaks are consistent throughout your file.
We recommend the Unix style for compatibility with Linux systems.


==== Binary zeros, BOM byte order mark and other non-text characters

Unusual characters or tool-specific formatting are sometimes hidden in application tools, but become apparent in plain-text editors.
If you come across these types of characters in your file, remove them entirely.


==== Test for data consistency

Using Cypher, you can see what is being imported and you can use that to your advantage.
`LOAD CSV` does not necessarily create a graph structure.
Instead, you can also only outputs samples, counts, or distributions to detect incorrect header column counts, delimiters, quotes, escapes, or header name spellings.

[source, cypher, role= nocopy noplay]
----
// Assert correct line count
LOAD CSV FROM "file-url" AS line
RETURN count(*);

// Check first 5 line-sample with header-mapping
LOAD CSV WITH HEADERS FROM "file-url" AS line
RETURN line
LIMIT 5;
----


=== Large amounts of data

`LOAD CSV` may timeout for files containing a significant number of rows (approaching hundreds of thousands or millions). For large files, it's recommended to split the import process in several lighter transactions through the clause xref:subqueries/subqueries-in-transactions.adoc[CALL {...} IN TRANSACTIONS].


The file link:https://data.neo4j.com/importing-cypher/persons.csv[_persons.csv_] contains a header line and a total of 869 lines with data about people.
The example loads the `name` and `born` columns in transactions of 200 rows.

.+persons.csv+
[source, csv, filename="persons.csv"]
----
person_tmdbId,bio,born,bornIn,died,person_imdbId,name,person_poster,person_url
3,"Legendary Hollywood Icon Harrison Ford was born on July 13, 1942 in Chicago, Illinois.   His family history includes a strong lineage of actors, radio personalities, and models.   Harrison attended public high school in Park Ridge, Illinois where he was a member of the school Radio Station WMTH.  Harrison worked as the lead voice for sports reporting at WMTH for several years.   Acting wasn’t a major interest to Ford until his junior year at Ripon College when he first took an acting class...",1942-07-13,"Chicago, Illinois, USA",,148,Harrison Ford,https://image.tmdb.org/t/p/w440_and_h660_face/5M7oN3sznp99hWYQ9sX0xheswWX.jpg,https://themoviedb.org/person/3
...
----

.Query
[source, cypher]
----
CALL {
  LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
  MERGE (p:Person)
  SET
  p.tmdbId = row.tmdbId,
  p.name = row.name,
  p.born = row.born
} IN TRANSACTIONS OF 200 ROWS
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 868
Properties set: 2604
Labels added: 868
Transactions committed: 5
----


=== Import data from relational databases

It is worthwhile to reason about your data model prior to importing data.
This holds especially for CSV data coming from a relational database. 
See link:https://neo4j.com/docs/getting-started/data-modeling/guide-data-modeling/[Data modeling].

Exporting data from a relational database to CSV may create one or multiple files, depending on whether the data are dumped as a single combined table or each table is dumbed as a aseparate file.


==== Import from a single CSV file

Consider the file link:https://data.neo4j.com/importing-cypher/books.csv[_books.csv_]:

.books.csv
[source, csv, filename="artists.csv"]
----
id,title,author,publication_year,genre,rating,still_in_print,last_purchased
19515,The Heights,Anne Conrad,2012,Comedy,5,true,2023/4/12 8:17:00
39913,Starship Ghost,Michael Tyler,1985,Science Fiction|Horror,4.2,false,2022/01/16 17:15:56
60980,The Death Proxy,Tim Brown,2002,Horror,2.1,true,2023/11/26 8:34:26
18793,Chocolate Timeline,Mary R. Robb,1924,Romance,3.5,false,2022/9/17 14:23:45
67162,Stories of Three,Eleanor Link,2022,Romance|Comedy,2,true,2023/03/12 16:01:23
25987,Route Down Below,Tim Brown,2006,Horror,4.1,true,2023/09/24 15:34:18
----

It contains both information about author nodes and book nodes.
You can execute multiple passes of `LOAD CSV` on a single file.
We even recommend you to do so for performance reasons, see <<_separate_creation_of_nodes_and_relationships>>.
Each pass of `LOAD CSV` focuses on the creation of one aspect of your data model and you import the relational data iteratively into Neo4j:

[source,cypher,role=noplay]
----
// Create `Book` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (b:Book {id: row.id})
SET b.title = row.title;

// Create `Author` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (a:Author {name: row.author});

// Create `WROTE` relationships
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MATCH (a:Author{name: row.author})
MATCH (b:Book{id: row.id})
MERGE (a)-[:WROTE]->(b);
----

==== Import from multiple CSV files

Consider the two files link:https://data.neo4j.com/importing-cypher/persons.csv[_persons.csv_] and link:https://data.neo4j.com/importing-cypher/movies.csv[_movies.csv_] from <<_create_constraints>> and the additional file link:https://data.neo4j.com/importing-cypher/acted_in.csv[_acted_in.csv_]:

.+acted_in.csv+
[source, csv, filename="acted_in.csv"]
----
movieId,person_tmdbId,role
1,12899,Slinky Dog (voice)
1,12898,Buzz Lightyear (voice)
...
----

The _acted_in.csv_ file contains data about the relationship between actors and the movies they acted in.
The connection between actors and movies is established by the properties `person_tmdbId` and `movieId`.
_acted_in.csv_ also holds the role the actor played in the movie.

The table represented by _acted_in.csv_ acts as a look-up table combining the primary keys, the IDs, of the tables represented by _persons.csv_ and _movies.csv_.

The following query contains three passes of `LOAD CSV` first creating person nodes from _persons.csv_ and movie nodes from _movies.csv_ and then the `ACTED_IN` relationship from _acted_in.csv_:

.Query
[source, cypher]
----
// Create person nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
MERGE (p:Person {tmdbId: row.person_tmdbId, name: row.name});

// Create movie nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/movies.csv' AS row
MERGE (m:Movie {movieId: row.movieId, title: row.title});

// Create relationships
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/acted_in.csv' AS row
MATCH (p:Person {tmdbId: row.person_tmdbId})
MATCH (m:Movie {movieId: row.movieId})
MERGE (p)-[r:ACTED_IN {role: row.role}]->(m);
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 961
Relationships created: 372
Properties set: 2294
----

For a fully fledged example, see link:https://neo4j.com/docs/getting-started/appendix/tutorials/guide-import-relational-and-etl/[Tutorial: Import data from a relational database into Neo4j].


== Process data during import


=== Cast CSV columns to Neo4j data types

`LOAD CSV` inserts all imported CSV data as string properties.
The file link:https://data.neo4j.com/importing-cypher/persons.csv[_persons.csv_] contains several columns which are not best represented by a string.
For example, values in the column `person_tmdbId` are integers, while values in the `born` column are dates.
To type cast the values while importing data, use the functions `toInteger()` and `date()`.

Neo4j has many more xref:values-and-types/casting-data.adoc[type-casting functions].
See xref:functions/temporal/index.adoc#functions-date[date()] and subsequent sections for more information about time-related type casting.

.+persons.csv+
[source, csv, filename="persons.csv"]
----
person_tmdbId,bio,born,bornIn,died,person_imdbId,name,person_poster,person_url
3,"Legendary Hollywood Icon Harrison Ford was born on July 13, 1942 in Chicago, Illinois.   His family history includes a strong lineage of actors, radio personalities, and models.   Harrison attended public high school in Park Ridge, Illinois where he was a member of the school Radio Station WMTH.  Harrison worked as the lead voice for sports reporting at WMTH for several years.   Acting wasn’t a major interest to Ford until his junior year at Ripon College when he first took an acting class...",1942-07-13,"Chicago, Illinois, USA",,148,Harrison Ford,https://image.tmdb.org/t/p/w440_and_h660_face/5M7oN3sznp99hWYQ9sX0xheswWX.jpg,https://themoviedb.org/person/3
...
----

.Query
[source, cypher]
----
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
MERGE (p:Person)
SET
p.tmdbId = toInteger(row.tmdbId),
p.name = row.name,
p.born = date(row.born)
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 868
Properties set: 2604
Labels added: 868
----

=== Handle null values

Neo4j does not store null values.
Null or empty fields in a CSV files can be skipped or replaced with default values in `LOAD CSV`.

Suppose you have this CSV file:

.companies.csv
[source]
----
Id,Name,Location,Email,BusinessType
1,Neo4j,San Mateo,contact@neo4j.com,P
2,AAA,,info@aaa.com,
3,BBB,Chicago,,G
----

Here are some examples of importing this data.

.Examples
[source,cypher,role=noplay]
----
// Skip null values
LOAD CSV WITH HEADERS FROM 'file:///companies.csv' AS row
WITH row WHERE row.Id IS NOT NULL
MERGE (c:Company {companyId: row.Id});

// Clear data
MATCH (n:Company) DELETE n;

// Set a default for null values
LOAD CSV WITH HEADERS FROM 'file:///companies.csv' AS row
MERGE (c:Company {companyId: row.Id, hqLocation: coalesce(row.Location, "Unknown")})

// Clear data
MATCH (n:Company) DELETE n;

// Change empty strings to null values (not stored)
LOAD CSV WITH HEADERS FROM 'file:///companies.csv' AS row
MERGE (c:Company {companyId: row.Id})
SET c.emailAddress = CASE trim(row.Email) WHEN "" THEN null ELSE row.Email END
----


=== Split list values

The file link:https://data.neo4j.com/importing-cypher/movies.csv[_movies.csv_] contains a header line and a total of 94 lines with data about movies.
Two columns contain list values, `languages` and `genres`:

.+movies.csv+
[source, csv, filename="movies.csv"]
----
movieId,title,budget,countries,movie_imdbId,imdbRating,imdbVotes,languages,plot,movie_poster,released,revenue,runtime,movie_tmdbId,movie_url,year,genres
1,Toy Story,30000000.0,USA,114709,8.3,591836,English,A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.,https://image.tmdb.org/t/p/w440_and_h660_face/uXDfjJbdP4ijW5hWSBrPrlKpxab.jpg,1995-11-22,373554033.0,81,862,https://themoviedb.org/movie/862,1995,Adventure|Animation|Children|Comedy|Fantasy
2,Jumanji,65000000.0,USA,113497,6.9,198355,English|French,"When two kids find and play a magical board game, they release a man trapped for decades in it and a host of dangers that can only be stopped by finishing the game.",https://image.tmdb.org/t/p/w440_and_h660_face/vgpXmVaVyUL7GGiDeiK1mKEKzcX.jpg,1995-12-15,262797249.0,104,8844,https://themoviedb.org/movie/8844,1995,Adventure|Children|Fantasy
...
----

Both lists are separated by the character `|`.
Use the `split()` function to separate the single values and create a list while importing the data:

.Query
[source, cypher]
----
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/movies.csv' AS row
MERGE (m:Movie)
SET
m.movieId = toInteger(row.movieId),
m.title = row.title,
m.imdbId = toInteger(row.movie_imdbId),
m.languages = split(row.languages, '|'),
m.genres = split(row.genres, '|')
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 93
Properties set: 465
Labels added: 93
----

See also xref:functions/string.adoc[String functions] for more options to work with string data.


=== Create additional node labels

The `ACTED_IN` relationship created in <<_create_relationships>> implicitly defines actors as a subset of people in _persons.csv_.
To apply an additional actor node label where it is applicable, based on the relationship:

.Query
[source, cypher]
----
MATCH (p:Person)-[:ACTED_IN]->()
WITH DISTINCT p SET p:Actor
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Labels added: 104
----

By adding the `Actor` label to the relevant person nodes, queries which target the label rather than the relationship are quicker to return, see xref:appendix/tutorials/basic-query-tuning.adoc[Basic query tuning].

== CSV file format

The CSV file format and `LOAD CSV` interact as follows:

* the character encoding is UTF-8;
* the line terminator is system dependent, for example, it is `\n` for Unix and `\r\n` for Windows;
* the default field delimiter is `,`. Change it using the option `FIELDTERMINATOR`;
* CSV files may contain quoted string values, wrapped in double quotes `"`, but the quotes are dropped when `LOAD CSV` reads the data;
* if `dbms.import.csv.legacy_quote_escaping` is `true`, `\` is the escape character; to escape a double quote, it must be in a quoted string and escaped, either with the escape character or a second double quote.


=== Headers

If the CSV file starts with a header row containing column names, each import row in the file acts as a map instead of an array.
Indicate the presence of the header row by adding `WITH HEADERS` to the query.
This way, you can access specific fields by their corresponding column name:

.artists-with-headers.csv
[source, csv, filename="artists-with-headers.csv"]
----
Id,Name,Year
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV WITH HEADERS FROM 'file:///artists-with-headers.csv' AS row
MERGE (:Artist {name: row.Name, year: toInteger(row.Year)})
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----

=== Field delimiter

The default field delimiter is `,`.
Use the `FIELDTERMINATOR` option to specify a different field delimiter.

.artists-fieldterminator.csv
[source, csv, filename="artists-fieldterminator.csv"]
----
1;ABBA;1992
2;Roxette;1986
3;Europe;1979
4;The Cardigans;1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists-fieldterminator.csv' AS row FIELDTERMINATOR ';'
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----

If you don't change the field delimiter character, `LOAD CSV` interprets the example CSV as having a single column without delimiters.
The first row, for example, is read as the string `'1;ABBA;1992'`. 

[NOTE]
====
You can use the hexadecimal representation of the unicode character for the field delimiter if you prepend `{backslash}u`.
Write the encoding with four digits, for example, `{backslash}u003B` is equivalent to `;` (semicolon).
====


=== Character escaping and quotes

If the configuration setting `dbms.import.csv.legacy_quote_escaping` is set to `true` (the default value), `\` is used as the escape character: `"The {backslash}"Symbol{backslash}""`.
The inner double quote characters are escaped, leaving them unprocessed by `LOAD CSV`.
For the double quote character, you can achieve the same thing by repeating it - the escape sequence above is equivalent to `"The ""Symbol"""`.

Quoted strings are allowed in the CSV file and the quotes are dropped when reading the data with `LOAD CSV`.
To apply quotation to a string, wrap it with double quote characters: `"my_string"`.

The example below has both additional quotes around each value as well as escaped quotes in the second value:

.artists-with-escaped-char.csv
[source, csv, filename="artists-with-escaped-char.csv"]
----
"1","The ""Symbol""","1992"
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists-with-escaped-char.csv' AS row
MERGE (a:Artist {name: row[1], year: toInteger(row[2])})
RETURN
  a.name AS name,
  a.year AS year,
  size(a.name) AS size
----

Note that `name` is a string and that it is wrapped in single quotes in the output.
The third column outputs the string length as `size`.
The length only counts what is between the single quotes, but not the quotes themselves:

.Result
[role="queryresult",options="header,footer",cols="3*<m"]
|===
| name | year | size
| 'The "Symbol"' | 1992 | 12
3+d| Nodes created: 1 +
Properties set: 2 +
Labels added: 1
|===


== Performance recommendations


=== Create and use indexes

xref:indexes/index.adoc[Indexes] can vastly speed up queries which is particularly useful if they are queried frequently. See xref:appendix/tutorials/basic-query-tuning.adoc[Basic query tuning].

Many types of constraints implicitly create indexes.
For example, Neo4j has already created indexes for the constraints from section <<_create_constraints>> for `person_tmdbId` and `movieId`.
The header of the file link:https://data.neo4j.com/importing-cypher/movies.csv[_movies.csv_] has more candidates for index properties: `movie_imdbId` and `title`.
Both could be regular query indexes.

To create indexes on the two properties:

.Query
[source, cypher]
----
CREATE INDEX movie_imdbId FOR (m:Movie) ON (m.movie_imdbId);
CREATE INDEX movie_title FOR (m:Movie) ON (m.title);
CALL db.awaitIndexes();
----


=== Separate creation of nodes and relationships

The file link:https://data.neo4j.com/importing-cypher/books.csv[_books.csv_] contains book data:

.books.csv
[source, csv, filename="artists.csv"]
----
id,title,author,publication_year,genre,rating,still_in_print,last_purchased
19515,The Heights,Anne Conrad,2012,Comedy,5,true,2023/4/12 8:17:00
39913,Starship Ghost,Michael Tyler,1985,Science Fiction|Horror,4.2,false,2022/01/16 17:15:56
60980,The Death Proxy,Tim Brown,2002,Horror,2.1,true,2023/11/26 8:34:26
18793,Chocolate Timeline,Mary R. Robb,1924,Romance,3.5,false,2022/9/17 14:23:45
67162,Stories of Three,Eleanor Link,2022,Romance|Comedy,2,true,2023/03/12 16:01:23
25987,Route Down Below,Tim Brown,2006,Horror,4.1,true,2023/09/24 15:34:18
----

The following Cypher query uses multiple MERGE clauses:

[source,cypher,role= nocopy noplay]
----
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (b:Book {id: row.id})
SET b.title = row.title
MERGE (a:Author {name: row.author})
MERGE (a)-[:WROTE]->(b)
----

If the data set was more complicated with significantly more rows, such a query might have experienced issues with the import as it creates related data in a single pass.
Instead, separate the creation of nodes and relationships:

[source,cypher,role=noplay]
----
// Create `Book` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (b:Book {id: row.id})
SET b.title = row.title;

// Create `Author` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (a:Author {name: row.author});

// Create `WROTE` relationships
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MATCH (a:Author{name: row.author})
MATCH (b:Book{id: row.id})
MERGE (a)-[:WROTE]->(b);
----


=== Avoid `Eager` operators

Some Cypher statements, for example, the first query in <<_separate_creation_of_nodes_and_relationships>>, pull in more rows than necessary, adding extra processing up front.
To avoid this, you can run link:https://neo4j.com/docs/cypher-manual/current/query-tuning/#how-do-i-profile-a-query[`PROFILE`] on your queries to see if they use `Eager` loading and either modify queries or run multiple passes on the same file, so it does not do this.
For more information about the `Eager` operator, see the link:https://neo4j.com/docs/cypher-manual/current/planning-and-tuning/operators/operators-detail/[Cypher manual -> Execution plan operators in detail^].


=== Database heap and memory

To help handle larger volumes of transactions, you can increase some configuration settings for the database and restart the instance for them to take effect.
You can create or update one million records in a single transaction per 2 GB of heap.

In `neo4j.conf`:
+
* `server.memory.heap.initial_size` and `server.memory.heap.max_size`: set to at least 4G.
* `server.memory.pagecache.size`: ideally, value large enough to keep the whole database in memory.


== Full example

You can reset all data in the database by running a series of DELETE and DROP queries:

.Query
[source, cypher]
----
MATCH (n) DETACH DELETE n;

DROP CONSTRAINT Person_tmdbId IF EXISTS;
DROP CONSTRAINT Movie_movieId IF EXISTS;
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Deleted 961 nodes, deleted 372 relationships.
Removed 2 constraints.
----

Deletion and creation can be combined into a single process consisting of multiple Cypher queries.

The full example combines queries from previous sections.

You can run this query at any point to refresh the database with the latest data, for example, if one of the CSV files is updated.
A single process to build your graph provides a consistent mechanism to test your import.

.Query
[source, cypher]
----
// Clear data
MATCH (n) DETACH DELETE n;

// Drop constraints
DROP CONSTRAINT Person_tmdbId IF EXISTS;
DROP CONSTRAINT Movie_movieId IF EXISTS;

// Create constraints
CREATE CONSTRAINT Person_tmdbId IF NOT EXISTS
FOR (p:Person)
REQUIRE p.tmdbId IS UNIQUE;

CREATE CONSTRAINT Movie_movieId IF NOT EXISTS
FOR (m:Movie)
REQUIRE m.movieId IS UNIQUE;

// Create person nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
MERGE (p:Person)
SET
p.tmdbId = toInteger(row.person_tmdbId),
p.name = row.name,
p.born = date(row.born);

// Create movie nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/movies.csv' AS row
MERGE (m:Movie)
SET
m.movieId = toInteger(row.movieId),
m.title = row.title,
m.imdbId = toInteger(row.movie_imdbId),
m.languages = split(row.languages, '|'),
m.genres = split(row.genres, '|');

// Create relationships
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/acted_in.csv' AS row
MATCH (p:Person {tmdbId: toInteger(row.person_tmdbId)})
MATCH (m:Movie {movieId: toInteger(row.movieId)})
MERGE (p)-[r:ACTED_IN]->(m)
SET r.role = row.role;

// Set additional node label
MATCH (p:Person)-[:ACTED_IN]->()
WITH DISTINCT p SET p:Actor;
----

.Result
[role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Added 2 constraints.
Nodes created: 961
Relationships created: 372
Properties set: 3441
Labels added: 1065
----

== Other ways of importing data

There are a few other approaches to get CSV data into Neo4j:

1. The `neo4j-admin database import` command: command-line tool useful for straightforward loading of large datasets.
_Works with Neo4j Desktop, Neo4j EE Docker image and local installations._
2. Neo4j ETL tool: Neo4j Labs project.
For more details and documentation, visit link:https://neo4j.com/labs/etl-tool/[Neo4j ETL Tool page].
3. Kettle import tool: maps and executes steps for the data process flow and works well for very large datasets, especially if you are already familiar with using this tool. _Works with any setup, including AuraDB._

Finally, avoiding `LOAD CSV` and deferring the parsing and querying of CSV data to your own application is a valid choice.