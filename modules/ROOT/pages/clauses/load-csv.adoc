:description: `LOAD CSV` is used to import data from CSV files into a Neo4j database.

= LOAD CSV

`LOAD CSV` is used to import data from CSV files.

[source, cypher]
----
LOAD CSV
FROM 'https://data.neo4j.com/bands/artists.csv' // <1>
AS row  // <2>
MERGE (:Artist {name: row[1], year: toInteger(row[2])})  // <3>
----

<1> `FROM` takes a string containing the path where the CSV file is located.
<2> The clause parses one row at a time, temporarily storing the current row in the variable name specified with `AS`.
<3> Use the row variable to access data in subsequent clauses, which may process and insert it into the database.

`LOAD CSV` supports both local and remote URLs.
Local paths are resolved relative to the Neo4j installation folder.

[NOTE]
====
Your user needs link:{neo4j-docs-base-uri}/operations-manual/{page-version}/authentication-authorization/load-privileges/[load privileges] to run `LOAD CSV` statements.
====

== Import CSV data into Neo4j

=== Import local files

You can store CSV files on the database server and then access them by using a `+file:///+` URL. By default, paths are resolved relative to the Neo4j installation folder.

.Import artists name and year information from a local file
====

.artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists.csv' AS row
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----
====

[NOTE]
It's not possible to use `LOAD CSV` with file URLs against an Aura instance, only remote URLs are supported.
For other ways of importing data, see link:https://neo4j.com/docs/aura/auradb/importing/importing-data/[Aura -> Importing data].

[TIP]
When using `+file:///+` URLs, spaces and other non-alphanumeric characters must be link:https://developer.mozilla.org/en-US/docs/Glossary/percent-encoding[URL-encoded].


==== Configuration settings for file URLs

link:{neo4j-docs-base-uri}/operations-manual/{page-version}/configuration/configuration-settings#config_dbms.security.allow_csv_import_from_file_urls[dbms.security.allow_csv_import_from_file_urls]::
This setting determines whether `+file:///+` URLs are allowed.

link:{neo4j-docs-base-uri}/operations-manual/{page-version}/configuration/configuration-settings#config_server.directories.import[server.directories.import]::
This setting sets the root directory relative to which `+file:///+` URLs are parsed.


=== Import from a remote location

You can import data from a CSV file hosted on a remote path.

`LOAD CSV` supports accessing CSV files via HTTPS, HTTP, and FTP.
It also follows redirects, except those changing the protocol (for security reasons).

.Import artists name and year information from a remote file
====

.https://data.neo4j.com/bands/artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'https://data.neo4j.com/bands/artists.csv' AS row
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----
====


=== Import compressed CSV files

`LOAD CSV` can read local CSV files compressed with ZIP or gzip.
ZIP archives can have arbitrary directory structures but may only contain a single CSV file. (so it travels down the dir tree? I gather it doesn't hold for gzip?)

.Import a CSV file from within a ZIP file
[source, cypher]
----
LOAD CSV FROM 'file:///artists.zip' AS row
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----


=== Create uniqueness constraints

Always create uniqueness xref:constraints/index.adoc[constraints] prior to importing data, to avoid duplicates or colliding entities.
If the source file contains duplicated data and the right constraints are in place, Cypher will raise an error (skipping only the duplicated rows, or aborting the whole import?)

.Create xref:constraints/examples.adoc#constraints-examples-node-uniqueness[node property uniqueness constraints] on person ID
====
(do we need to call the property so ugly?)

.persons.csv
[source, csv, filename="persons.csv"]
----
person_tmdbId,bio,born,bornIn,died,person_imdbId,name,person_poster,person_url
3,"Legendary Hollywood Icon Harrison Ford was born on July 13, 1942 in Chicago, Illinois.   His family history includes a strong lineage of actors, radio personalities, and models.   Harrison attended public high school in Park Ridge, Illinois where he was a member of the school Radio Station WMTH.  Harrison worked as the lead voice for sports reporting at WMTH for several years.   Acting wasn’t a major interest to Ford until his junior year at Ripon College when he first took an acting class...",1942-07-13,"Chicago, Illinois, USA",,148,Harrison Ford,https://image.tmdb.org/t/p/w440_and_h660_face/5M7oN3sznp99hWYQ9sX0xheswWX.jpg,https://themoviedb.org/person/3
...
----

.Create a node property uniqueness constraint on person ID
[source, cypher]
----
CREATE CONSTRAINT Person_tmdbId IF NOT EXISTS
FOR (p:Person)
REQUIRE p.tmdbId IS UNIQUE
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Added 1 constraint.
----
====


=== Handle large amounts of data

`LOAD CSV` may timeout for files containing a significant number of rows (approaching hundreds of thousands or millions). For large files, it's recommended to split the import process in several lighter transactions through the clause xref:subqueries/subqueries-in-transactions.adoc[`CALL {...} IN TRANSACTIONS`].

.Load a large CSV file in several transactions
====
The file link:https://data.neo4j.com/importing-cypher/persons.csv[_persons.csv_] contains a header line and a total of 869 lines.
The example loads the `name` and `born` columns in transactions of 200 rows.

.persons.csv
[source, csv]
----
person_tmdbId,bio,born,bornIn,died,person_imdbId,name,person_poster,person_url
3,"Legendary Hollywood Icon Harrison Ford was born on July 13, 1942 in Chicago, Illinois.   His family history includes a strong lineage of actors, radio personalities, and models.   Harrison attended public high school in Park Ridge, Illinois where he was a member of the school Radio Station WMTH.  Harrison worked as the lead voice for sports reporting at WMTH for several years.   Acting wasn’t a major interest to Ford until his junior year at Ripon College when he first took an acting class...",1942-07-13,"Chicago, Illinois, USA",,148,Harrison Ford,https://image.tmdb.org/t/p/w440_and_h660_face/5M7oN3sznp99hWYQ9sX0xheswWX.jpg,https://themoviedb.org/person/3
...
----

.Query
[source, cypher]
----
CALL {
  LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
  MERGE (p:Person)
  SET
  p.tmdbId = row.tmdbId,
  p.name = row.name,
  p.born = row.born
} IN TRANSACTIONS OF 200 ROWS
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 868
Properties set: 2604
Labels added: 868
Transactions committed: 5
----
====


=== Import data from relational databases

If the source data comes from a relational model, it's worth evaluating whether there's anything to gain in the move to a graph data model.
Before running the import, think about how the data can be modeled as a graph, and adapt its structure accordingly when running the import (see link:https://neo4j.com/docs/getting-started/data-modeling/guide-data-modeling/[Graph data modeling]).

Data from relational databases may consist in one or multiple CSV files, depending on the source database structure.
In either cases, the most performant approach is to run multiple passes of `LOAD CSV` to import nodes separately from relationships.


.Import from a single CSV file
====
The source file link:https://data.neo4j.com/importing-cypher/books.csv[_books.csv_] contains information about both authors and books.
From a graph perspective, these are nodes with different labels, so it takes different queries to load them.

The example executes multiple passes of `LOAD CSV` on that one file, and each pass focuses on the creation of _one_ entity.
This is the best performant practice, see <<_separate_creation_of_nodes_and_relationships>>.

.books.csv
[source, csv]
----
id,title,author,publication_year,genre,rating,still_in_print,last_purchased
19515,The Heights,Anne Conrad,2012,Comedy,5,true,2023/4/12 8:17:00
39913,Starship Ghost,Michael Tyler,1985,Science Fiction|Horror,4.2,false,2022/01/16 17:15:56
60980,The Death Proxy,Tim Brown,2002,Horror,2.1,true,2023/11/26 8:34:26
18793,Chocolate Timeline,Mary R. Robb,1924,Romance,3.5,false,2022/9/17 14:23:45
67162,Stories of Three,Eleanor Link,2022,Romance|Comedy,2,true,2023/03/12 16:01:23
25987,Route Down Below,Tim Brown,2006,Horror,4.1,true,2023/09/24 15:34:18
----

.Query
[source, cypher]
----
// Create `Book` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (b:Book {id: row.id})
SET b.title = row.title;
// why?

// Create `Author` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (a:Author {name: row.author});

// any loss in merging the two previous queries?

// Create `WROTE` relationships
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MATCH (a:Author{name: row.author})
MATCH (b:Book{id: row.id})
MERGE (a)-[:WROTE]->(b);
----

no result?

====


.Import from multiple CSV files
====
The file link:https://data.neo4j.com/importing-cypher/acted_in.csv[_acted_in.csv_] contains data about the relationship between actors and the movies they acted in (from link:https://data.neo4j.com/importing-cypher/persons.csv[_persons.csv_] and link:https://data.neo4j.com/importing-cypher/movies.csv[_movies.csv_]).
Actors and movies are linked through their ID columns `person_tmdbId` and `movieId`.

The file also holds the role the actor played in the movie, and is imported in Neo4j as a relationship property.

.acted_in.csv
[source, csv, filename="acted_in.csv"]
----
movieId,person_tmdbId,role
1,12899,Slinky Dog (voice)
1,12898,Buzz Lightyear (voice)
...
----

It takes three `LOAD CSV` passes to import this dataset: the first two  create `Person` nodes from _persons.csv_ and `Movie` nodes from _movies.csv_, and the third adds the `:ACTED_IN` relationship from _acted_in.csv_.

.Query
[source, cypher]
----
// Create person nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
MERGE (p:Person {tmdbId: row.person_tmdbId, name: row.name});

// Create movie nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/movies.csv' AS row
MERGE (m:Movie {movieId: row.movieId, title: row.title});

// Create relationships
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/acted_in.csv' AS row
MATCH (p:Person {tmdbId: row.person_tmdbId})
MATCH (m:Movie {movieId: row.movieId})
MERGE (p)-[r:ACTED_IN {role: row.role}]->(m);
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 961
Relationships created: 372
Properties set: 2294
----
====

[TIP]
For a guide on importing the Northwind dataset from Postgres into Neo4j, see link:https://neo4j.com/docs/getting-started/appendix/tutorials/guide-import-relational-and-etl/[Tutorial: Import data from a relational database into Neo4j].


==== Create additional node labels

In Neo4j a node can have multiple labels, while in a relational setting it's not as straightforward to mix entities.
For example, a node in Neo4j can be labelled both as `Dog` and `Actor`, while in a relational model dogs and actors are separate entities.

After a relational dataset has been imported, there may be further labels that can be added, depending on the use-case you intend to use the dataset for.
Labels are one of the most performant ways of pinpointing a node, so adding more of them can be beneficial for performance.

.Add extra `Actor` label on `Person` nodes
====
The `:ACTED_IN` relationship from _acted_in.csv_ implicitly defines actors as a subset of people.
The following queries adds an additional `Actor` label to all people who have acted in something.

.Query
[source, cypher]
----
MATCH (p:Person)-[:ACTED_IN]->()
WITH DISTINCT p SET p:Actor
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Labels added: 104
----
====


== Pre-process the data during import


=== Cast CSV columns to Neo4j data types

`LOAD CSV` inserts all imported CSV data as string properties.
Neo4j supports however a range of xref:values-and-types/index.adoc[data types], and storing data in its most appropriate type allows both to query it more effectively and to process it with type-specific Cypher functions.
For these reasons, there's value in casting non-string values to the relevant Cypher types while importing it.

.Import numeric and temporal data
====
The column `person_tmdbId` and `born` in the file link:https://data.neo4j.com/importing-cypher/persons.csv[_persons.csv_] contains integers and dates respectively.
The functions `toInteger()` and `date()` allow to cast those values to the appropriate types before importing them.

.persons.csv
[source, csv]
----
person_tmdbId,bio,born,bornIn,died,person_imdbId,name,person_poster,person_url
3,"Legendary Hollywood Icon Harrison Ford was born on July 13, 1942 in Chicago, Illinois.   His family history includes a strong lineage of actors, radio personalities, and models.   Harrison attended public high school in Park Ridge, Illinois where he was a member of the school Radio Station WMTH.  Harrison worked as the lead voice for sports reporting at WMTH for several years.   Acting wasn’t a major interest to Ford until his junior year at Ripon College when he first took an acting class...",1942-07-13,"Chicago, Illinois, USA",,148,Harrison Ford,https://image.tmdb.org/t/p/w440_and_h660_face/5M7oN3sznp99hWYQ9sX0xheswWX.jpg,https://themoviedb.org/person/3
...
----

.Query
[source, cypher]
----
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
MERGE (p:Person)
SET
p.tmdbId = toInteger(row.tmdbId),
p.name = row.name,
p.born = date(row.born)
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 868
Properties set: 2604
Labels added: 868
----
====

For a list of type casting functions, see xref:values-and-types/casting-data.adoc[Casting data values].


=== Handle null values

Neo4j does not store null values.
Null or empty fields in a CSV files can be skipped or replaced with default values in `LOAD CSV`.

.Processing a file with null values
====
In the file `companies.csv`, some rows do not specify values for some columns.
The examples show several options of treating null values.

.companies.csv
[source, csv, filename=companies.csv]
----
Id,Name,Location,Email,BusinessType
1,Neo4j,San Mateo,contact@neo4j.com,P
2,AAA,,info@aaa.com,
3,BBB,Chicago,,G
----

.Skip null values
[source, cypher]
----
LOAD CSV WITH HEADERS FROM 'file:///companies.csv' AS row
WITH row WHERE row.Id IS NOT NULL
MERGE (c:Company {companyId: row.Id});
----

.Provide a default for null values
[source, cypher]
----
LOAD CSV WITH HEADERS FROM 'file:///companies.csv' AS row
MERGE (c:Company {companyId: row.Id, hqLocation: coalesce(row.Location, "Unknown")})
----

.Change empty strings to null values (not stored)
[source, cypher]
----
LOAD CSV WITH HEADERS FROM 'file:///companies.csv' AS row
MERGE (c:Company {companyId: row.Id})
SET c.emailAddress = CASE trim(row.Email) WHEN "" THEN null ELSE row.Email END
----
====

[TIP]
Because null values are not stored in the database, a strategy for selectively getting rid of some values is to map them into null values.


=== Split list values

The function `split()` allows to convert a string of elements into a list.

.Parse movies languages and genres as lists
====
The file link:https://data.neo4j.com/importing-cypher/movies.csv[_movies.csv_] contains a header line and a total of 94 lines.

The columns `languages` and `genres` contain list-like values.
Both lists are separated by a pipe `|`, and `split()` allows to make them into Cypher lists ahead of inserting them into the database.

.movies.csv
[source, csv]
----
movieId,title,budget,countries,movie_imdbId,imdbRating,imdbVotes,languages,plot,movie_poster,released,revenue,runtime,movie_tmdbId,movie_url,year,genres
1,Toy Story,30000000.0,USA,114709,8.3,591836,English,A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.,https://image.tmdb.org/t/p/w440_and_h660_face/uXDfjJbdP4ijW5hWSBrPrlKpxab.jpg,1995-11-22,373554033.0,81,862,https://themoviedb.org/movie/862,1995,Adventure|Animation|Children|Comedy|Fantasy
2,Jumanji,65000000.0,USA,113497,6.9,198355,English|French,"When two kids find and play a magical board game, they release a man trapped for decades in it and a host of dangers that can only be stopped by finishing the game.",https://image.tmdb.org/t/p/w440_and_h660_face/vgpXmVaVyUL7GGiDeiK1mKEKzcX.jpg,1995-12-15,262797249.0,104,8844,https://themoviedb.org/movie/8844,1995,Adventure|Children|Fantasy
...
----

.Query
[source, cypher]
----
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/movies.csv' AS row
MERGE (m:Movie)
SET
m.movieId = toInteger(row.movieId),
m.title = row.title,
m.imdbId = toInteger(row.movie_imdbId),
m.languages = split(row.languages, '|'),
m.genres = split(row.genres, '|')
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 93
Properties set: 465
Labels added: 93
----
====

For more string manipulation functions, see xref:functions/string.adoc[String functions].


== Neo4j CSV functions

=== Access line numbers with `linenumber()`

The xref:functions/load-csv.adoc#functions-linenumber[`linenumber()`] function provides the line number which `LOAD CSV` is operating on, or `null` if called out of a `LOAD CSV` context.

A common use case for this function is to generate sequential unique IDs for CSV data that doesn't have a unique column already.

.artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists.csv' AS row
RETURN linenumber() AS number, row
----

.Result
[source, role="queryresult"]
----
+---------------------------------------+
| number | row                          |
+---------------------------------------+
| 1      | ["1","ABBA","1992"]          |
| 2      | ["2","Roxette","1986"]       |
| 3      | ["3","Europe","1979"]        |
| 4      | ["4","The Cardigans","1992"] |
+---------------------------------------+
4 rows
----


=== Access the CSV file path with `file()`

The xref:functions/load-csv.adoc#functions-file[`file()`] function provides the absolute path of the file that `LOAD CSV` is operating on, or `null` if called out of a `LOAD CSV` context.

.artists.csv
[source, csv, filename="artists.csv"]
----
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher, role=test-result-skip]
----
LOAD CSV FROM 'file:///artists.csv' AS row
RETURN DISTINCT file() AS path
----

.Result
[source, role="queryresult"]
----
+------------------------------------------+
| path                                     |
+------------------------------------------+
| "/home/example/neo4j/import/artists.csv" |
+------------------------------------------+
1 row
----

[TIP]
`file()` always returns a local path, even when loading remote CSV files.
For remote resources, `file()` returns the temporary local path it was downloaded to.


== CSV file format

The CSV file format and `LOAD CSV` interact as follows:

* the file character encoding must be UTF-8;
* the line terminator is system dependent (`\n` for Unix and `\r\n` for Windows);
* the default field delimiter is `,`. Change it with the option `FIELDTERMINATOR`;
* CSV files may contain quoted string values, and the quotes are dropped when `LOAD CSV` reads the data;
* if `dbms.import.csv.legacy_quote_escaping` is set to the default value of `true`, `\` is used as an escape character;
* a double quote must be in a quoted string and escaped, with either the escape character or a second double quote.


=== Headers

If the CSV file starts with a header row containing column names, each import row in the file acts as a map instead of an array.

You must indicate the presence of the header row by adding `WITH HEADERS` to the query.
You can then access specific fields by their corresponding column name.

.Parsing a CSV as a list of maps
====
.artists-with-headers.csv
[source, csv, filename="artists-with-headers.csv"]
----
Id,Name,Year
1,ABBA,1992
2,Roxette,1986
3,Europe,1979
4,The Cardigans,1992
----

.Query
[source, cypher]
----
LOAD CSV WITH HEADERS FROM 'file:///artists-with-headers.csv' AS row
MERGE (:Artist {name: row.Name, year: toInteger(row.Year)})
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----
====


=== Field delimiter

The default field delimiter is `,`.
Use the `FIELDTERMINATOR` option to specify a different field delimiter.

If you try to import a file that doesn't use `,` as field delimiter, but don't specify a custom delimiter, `LOAD CSV` will interpret the example CSV as having a single column. (but multiple rows?)

.Import a CSV using `;` as field delimiter
====
.artists-fieldterminator.csv
[source, csv, filename="artists-fieldterminator.csv"]
----
1;ABBA;1992
2;Roxette;1986
3;Europe;1979
4;The Cardigans;1992
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists-fieldterminator.csv' AS row FIELDTERMINATOR ';'
MERGE (:Artist {name: row[1], year: toInteger(row[2])})
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Nodes created: 4
Properties set: 8
Labels added: 4
----
====


[NOTE]
You can use the hexadecimal representation of the unicode character for the field delimiter if you prepend `{backslash}u`.
Write the encoding with four digits: for example, `{backslash}u003B` is equivalent to `;` (semicolon).


=== Quotes escaping

Quoted strings are allowed in the CSV file and the quotes are dropped when `LOAD CSV` reads the data. If quoted strings need to contain quote characters `"`, there are two ways to escape them:

1. *Double quotes* -- Use another quote `"` to escape a quote (for example, the CSV encoding of the string `The "Symbol"` is `"The ""Symbol"""`).
2. *Prefix with backslash `\`* -- If the configuration setting `dbms.import.csv.legacy_quote_escaping` is set to `true` (the default value), `\` works as the escape character for quotes (for example, the CSV encoding of the string `The "Symbol"` is `"The {backslash}"Symbol{backslash}""`).

.Import a CSV with double-quotes escaping
====
.artists-with-escaped-quotes.csv
[source, csv, filename="artists-with-escaped-quotes.csv"]
----
"1","The ""Symbol""","1992"
"1","The \"Symbol\"","1992"
----

.Query
[source, cypher]
----
LOAD CSV FROM 'file:///artists-with-escaped-quotes.csv' AS row
MERGE (a:Artist {name: row[1], year: toInteger(row[2])})
RETURN
  a.name AS name,
  a.year AS year,
  size(a.name) AS size
----

.Result
[source, role="queryresult",options="header,footer",cols="3*<m"]
|===
| name | year | size
| 'The "Symbol"' | 1992 | 12
| 'The "Symbol"' | 1992 | 12
3+d| Nodes created: 2 +
Properties set: 4 +
Labels added: 1
|===

Note that `name` is a string, as it is wrapped in single quotes in the output.
The third column outputs the string length as `size`.
The length only counts what is between the single quotes, but not the quotes themselves.

====


=== Check source data quality

Especially in case of a failed import, there are some elements to check to ensure the source file is not corrupted.

- *Inconsistent headers* -- The CSV header may be inconsistent with the data.
It can be missing, have too many columns or have a different delimiter.
Verify that the header matches the data in the file.
Adjust the formatting, delimiters or columns.
- *Extra or missing quotes* -- Standalone double or single quotes in the middle of non-quoted text or non-escaped quotes in quoted text can cause issues reading the file.
Either escape or remove stray quotes.
See <<Quotes escaping>>.
- *Special or newline characters* -- When dealing with special characters in a file, ensure they are quoted or remove them.
- *Inconsistent line breaks* -- Ensure line breaks are consistent throughout your file.
We recommend the Unix style for compatibility with Linux systems. (and is compatible with windows? does it even matter, is not up to the jvm to do the parsing, which is system agnostic?)
- *Binary zeros, BOM byte order mark and other non-text characters* -- Unusual characters or tool-specific formatting are sometimes hidden in application tools, but become apparent in plain-text editors.
If you come across these types of characters in your file, remove them entirely.


==== Inspect source files ahead of import

Before actually importing data into the database, you can use `LOAD CSV` to inspect a source file and get an idea of what form the imported data is going to have.

[source, cypher]
----
// Assert correct line count
LOAD CSV FROM "file-url" AS line
RETURN count(*);

// Check first 5 line-sample with header-mapping
LOAD CSV WITH HEADERS FROM "file-url" AS line
RETURN line
LIMIT 5;
----

put a remote url and result of queries?


== Performance recommendations


=== Create and use indexes

xref:indexes/index.adoc[Indexes] can vastly speed up queries targeting the indexes entities.
Either before or after the import, create indexes on the labels and properties that you expect to query most often.

.Create indexes on movie ID and title
[source, cypher]
----
CREATE INDEX movie_imdbId FOR (m:Movie) ON (m.movie_imdbId) IF NOT EXISTS;
CREATE INDEX movie_title FOR (m:Movie) ON (m.title) IF NOT EXISTS;
----

For more information on the role of indexes, see xref:indexes/search-performance-indexes/using-indexes.adoc[].


=== Separate creation of nodes and relationships

It is more efficient to create _all_ nodes first, and then add relationships with a second pass.

.Comparison: import nodes and relationships together VS splitting them
====
.https://data.neo4j.com/importing-cypher/books.csv
[source, csv]
----
id,title,author,publication_year,genre,rating,still_in_print,last_purchased
19515,The Heights,Anne Conrad,2012,Comedy,5,true,2023/4/12 8:17:00
39913,Starship Ghost,Michael Tyler,1985,Science Fiction|Horror,4.2,false,2022/01/16 17:15:56
60980,The Death Proxy,Tim Brown,2002,Horror,2.1,true,2023/11/26 8:34:26
18793,Chocolate Timeline,Mary R. Robb,1924,Romance,3.5,false,2022/9/17 14:23:45
67162,Stories of Three,Eleanor Link,2022,Romance|Comedy,2,true,2023/03/12 16:01:23
25987,Route Down Below,Tim Brown,2006,Horror,4.1,true,2023/09/24 15:34:18
----

.Bad practice -- Create nodes and relationships at once
[source, cypher]
----
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (b:Book {id: row.id})
SET b.title = row.title
MERGE (a:Author {name: row.author})
MERGE (a)-[:WROTE]->(b)
----

merge vs create ?

.Good practice -- Separate nodes and relationships creation
[source, cypher]
----
// Create `Book` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (b:Book {id: row.id})
SET b.title = row.title;

// Create `Author` nodes
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MERGE (a:Author {name: row.author});

// Create `WROTE` relationships
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/importing-cypher/books.csv'
AS row
MATCH (a:Author{name: row.author})
MATCH (b:Book{id: row.id})
MERGE (a)-[:WROTE]->(b);
----

can we make this example with a significant dataset with many rows?

====


=== Avoid `Eager` operators

Depending on the exact shape of your Cypher query, the database may have to craft a query plan that contains the link:https://neo4j.com/docs/cypher-manual/current/planning-and-tuning/operators/operators-detail/#query-plan-eager[`Eager`] operator.
You may discover if this is the case by link:https://neo4j.com/docs/cypher-manual/current/query-tuning/#how-do-i-profile-a-query[profiling your queries], and attempt tweaking them to avoid it in case.


=== Database heap and memory

To help handle larger volumes of transactions, there are a few some memory configuration settings you may increase:

* `server.memory.heap.initial_size` and `server.memory.heap.max_size`: set to at least 4G.
* `server.memory.pagecache.size`: ideally, a value large enough to keep the whole database in memory.

As a rule of thumb, you can create or update one million records in a single transaction per 2 GB of heap. (source?)

links?


== A full example

.Erase current database and import the full movie dataset
[source, cypher]
----
// Clear data
MATCH (n) DETACH DELETE n;

// Create constraints
CREATE CONSTRAINT Person_tmdbId IF NOT EXISTS
FOR (p:Person)
REQUIRE p.tmdbId IS UNIQUE;

CREATE CONSTRAINT Movie_movieId IF NOT EXISTS
FOR (m:Movie)
REQUIRE m.movieId IS UNIQUE;

// Create person nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/persons.csv' AS row
MERGE (p:Person)
SET
p.tmdbId = toInteger(row.person_tmdbId),
p.name = row.name,
p.born = date(row.born);

// Create movie nodes
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/movies.csv' AS row
MERGE (m:Movie)
SET
m.movieId = toInteger(row.movieId),
m.title = row.title,
m.imdbId = toInteger(row.movie_imdbId),
m.languages = split(row.languages, '|'),
m.genres = split(row.genres, '|');

// Create relationships
LOAD CSV FROM 'https://data.neo4j.com/importing-cypher/acted_in.csv' AS row
MATCH (p:Person {tmdbId: toInteger(row.person_tmdbId)})
MATCH (m:Movie {movieId: toInteger(row.movieId)})
MERGE (p)-[r:ACTED_IN]->(m)
SET r.role = row.role;

// Set additional node label
MATCH (p:Person)-[:ACTED_IN]->()
WITH DISTINCT p SET p:Actor;
----

.Result
[source, role="queryresult"]
----
+-------------------+
| No data returned. |
+-------------------+
Added 2 constraints.
Nodes created: 961
Relationships created: 372
Properties set: 3441
Labels added: 1065
----


== Other ways of importing data

There are a few other tools to get CSV data into Neo4j.

1. *The link:https://neo4j.com/docs/operations-manual/5/tools/neo4j-admin/neo4j-admin-import/[`neo4j-admin database import`] command* is the most efficient way of importing large CSV files.
_Available only in Neo4j Desktop or Enteprise Edition instances._ actual?
2. Use a link:https://neo4j.com/docs/create-applications/[*language library*] to parse CSV data and run creation Cypher queries against a Neo4j database.
2. The *Neo4j ETL tool* allows to extract the schema from a relational database and turn it into a graph model. It then takes care of importing the data into Neo4j.
For more details and documentation, visit link:https://neo4j.com/labs/etl-tool/[Neo4j ETL Tool page].
3. The *Kettle import tool* maps and executes steps for the data process flow and works well for very large datasets, especially if you are already familiar with using this tool.
